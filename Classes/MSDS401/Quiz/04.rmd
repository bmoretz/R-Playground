---
title: 'Problem Set #4'
author: "Moretz_Brandon"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: yes
  html_document:
    df_print: paged
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r additional-libraries, echo=FALSE}

library(data.table, quietly = TRUE, warn.conflicts = FALSE)

assignInNamespace("cedta.pkgEvalsUserCode", c(data.table:::cedta.pkgEvalsUserCode, "rtvs"), "data.table")

library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)

theme_set(theme_light())

# Theme Overrides
theme_update(plot.title = element_text(hjust = 0.5),
			 axis.text.x = element_text(size = 10),
			 axis.text.y = element_text(size = 10),
			 axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
			 legend.position = "top", legend.title = element_blank())

pretty_vector <- function(vec, label = "") {
  pander::pander(vec)
}

```


## 1.)

True or False

The phrase "linear regression" pertains to regression models with normal equations that can be expressed in matrix form using linear algebra to determine coefficient estimates.

Answer:

__True__

## 2.)

Choose the one alternative that best completes the statement or answers the question.

Assume two independent random samples are available which provide sample proportions.

+ For the first sample assume __n1 = 100__ and __x1 = 39__. 
+ For the second sample, assume __n2 = 100__ and __x2 = 49__. 

Test the null hypothesis that the population proportions are equal versus the alternative hypothesis that the proportions are not equal at the 90% confidence level.   Frame the test statistic by subtracting the proportion for population 1 from that for population 2.  Pick an appropriate z value, p-value and conclusion.  Round your answer to the nearest thousandth.

```{r echo = T}

n1 <- 100
x1 <- 39

n2 <- 100
x2 <- 49

p <- (x1 + x2) / (n1 + n2)
std <- sqrt(p * (1 - p) * (1 / 100 + 1 / 100))
z <- (0.49 - 0.39) / std
z
1 - pnorm(z, 0, 1, lower.tail = T)

pretty_vector(round(p, 4))

alpha <- ( 1 - .9 ) / 2

crit.value <- qnorm(alpha, lower.tail = F)

ifelse(z > crit.value, "Reject Null", "Fail To Reject" )

```

## 3.)

Construct the indicated confidence interval for the difference between the two population means. 

Assume that the two samples are independent simple random samples selected from normally distributed populations. 
Assume that the population standard deviations are equal.

Two types of flares are tested and their burning times are recorded. The summary statistics are given below.

+ Brand X  __n = 35__, __mean = 19.4__ minutes, __standard deviation = 1.4__ minutes
+ Brand Y  __n = 40__,  __mean = 15.1__ minutes  __standard deviation = 1.3__ minutes

Construct a 95% confidence interval for the differences between the mean burning time of the brand X flare and the mean burning time of the brand Y flare.

```{r echo = T}

x.n <- 35
x.mu <- 19.4
x.sd <- 1.4

y.n <- 40
y.mu <- 15.1
y.sd <- 1.3

# Determine standard error
SE <- sqrt((x.sd ** 2 / x.n) + (y.sd ** 2 / y.n))

# Determine degrees of freedom
df <- (x.n - 1) + (y.n - 1)

# Determine t level for 95% CI @ DF
t <- qt((1 - 0.05 / 2), df)

ci.lower <- (x.mu - y.mu) - (t * SE)
ci.upper <- (x.mu - y.mu) + (t * SE)
pretty_vector(c(ci.lower, ci.upper), 1)

```


## 4.)

Construct a 95% confidence interval for the mean difference µd using a sample of paired data for which summary statistics are given. 
Assume the parent populations are normally distributed.  

Assume the sample mean difference __d = 3.0__, the sample standard deviation __sd = 2.911__, and __n = 8__.

```{r echo = T}

d <- 3
sd <- 2.911
n <- 8

SE <- (sd / sqrt(n))
df <- (n - 1)
t <- qt(1 - 0.05 / 2, df)

ci.lower <- (d) - (t * SE)
ci.upper <- (d) + (t * SE)

pretty_vector(c(ci.lower, ci.upper), 3)

```

## 5.)

One-way Analysis of Variance uses a F test to compare the variance between the treatment level means to the error variance (the pooled variance within levels).  For this test, 
one assumption is that the observations are random samples drawn from normally distributed populations with unequal variances. 

__False__

## 6.)

Use the given data to answer the question.

What is the critical value for the F- test at 95% confidence?

<table style="width: 382px; height: 106px;" border="1">
<tbody>
<tr>
<td>&nbsp;</td>
<td>DF</td>
<td>SS</td>
<td>MS</td>
<td>F</td>
<td>p</td>
</tr>
<tr>
<td>Factor</td>
<td>3</td>
<td>13.500</td>
<td>4.500</td>
<td>5.17</td>
<td>0.011</td>
</tr>
<tr>
<td>Error</td>
<td>16</td>
<td>13.925</td>
<td>0.870</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>Total</td>
<td>19</td>
<td>27.425</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table> 


```{r echo = T}

pretty_vector(round(qf(.95, 3, 16, lower.tail = T), 3), "Critical Value")

```

## 7.)

Solve the problem.

A manager at a bank is interested in comparing the standard deviation of the waiting times when a single waiting line is used versus when individual lines are used. He wishes to test the claim that the population standard deviation for waiting times when multiple lines are used is greater than the population standard deviation for waiting times when a single line is used. This is a test of differences in variability.  Find the p-value for a test of this claim given the following sample data. If you use R, you will be able to calculate a precise p-value.  If you do a table lookup, you won't be able to find the exact p-value, but will be able to bound the p-value.   Retain at least 4 digits in the calculated p-value if you use R.  

Sample 1: multiple waiting lines: n1 = 13, s1 = 2.0 minutes 
Sample 2: single waiting line: n2 = 16, s2 = 1.1 minutes

```{r echo = T}

#The statistic F=s1^2/s2^2 has an F distribution with (n1-1) and (n2-1) deg. freedom. 

s1 <- 2.1
s2 <- 1.1
f.calc <- (2.1 ^ 2) / (1.1 ^ 2)

f.table <- qf((1 - 0.01), 12, 15)

p <- f.table - f.calc

0.01 <= p && p <= 0.025

```


## 8.)

Provide an appropriate response.

Fill in the missing entries in the following partially completed one-way ANOVA table.

<table style="width: 472px; height: 106px;" border="1">
<tbody>
<tr>
<td>Source</td>
<td>df</td>
<td>SS</td>
<td>MS-SS/df</td>
<td>F-statistic</td>
</tr>
<tr>
<td>Treatment</td>
<td>&nbsp;</td>
<td>22.2</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>Error</td>
<td>26</td>
<td>&nbsp;</td>
<td>4</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>Total</td>
<td>31</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>

Answer:

<table style="width: 472px; height: 106px;" border="1">
<tbody>
<tr>
<td>Source</td>
<td>df</td>
<td>SS</td>
<td>MS-SS/df</td>
<td>F-statistic</td>
</tr>
<tr>
<td>Treatment</td>
<td>5</td>
<td>22.2</td>
<td>4.44</td>
<td>1.11</td>
</tr>
<tr>
<td>Error</td>
<td>26</td>
<td>104</td>
<td>4</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>Total</td>
<td>31</td>
<td>126.2</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>

```{r}

31 - 26 # treatment df

22.2 / 5 # treatment MS-SS/df 

26 * 4 # ss error

104 + 22.2 # ss total

4.44 / 4 # f-statistic

```

## 9.)

Use the given data to find the equation of the regression line.  

Round the final values to three significant digits, if necessary.  

Let x be the independent variable and y the dependent variable.  (Note that if x = 2, then y = 7 and so forth.  yhat is the predicted value of the fitted equation.)

x   2    4    5    6  8

y   7  11  13   20  24

```{r}

data <- data.table(x = c(2, 4, 5, 6, 8), y = c(7, 11, 13, 20, 24))

n <- 5

xhat <- sum(1 / n * data$x)
yhat <- sum(1 / n * data$y)

sx <- round(sqrt(sum(1 / (n - 1) * (data$x - xhat) ** 2)), 3)
sy = round(sqrt(sum(1 / (n - 1) * (data$y - yhat) ** 2)), 3)

sxy <- sum(data$x * data$y)

sxyn <- (sum(data$x) * sum(data$y)) / n

sshat <- sum( ( data$x - xhat) ** 2 ) * sum( ( data$y - yhat) ** 2)

b1 <- round( ( sxy - sxyn ) / sqrt( sshat ), 4 )

y_hat <- round(b1 * ( sy / sx ), 4 )

y_hat

```

## 10.)

If a linear correlation coefficient (Pearson correlation) is -0.8443 for data suitable for simple linear regression, then the slope of the simple linear regression line must be negative 
and the r-squared value is 0.8443.


__False__

The slope will also be negative since the correlation is negative, but the R2 does not equal 0.8443